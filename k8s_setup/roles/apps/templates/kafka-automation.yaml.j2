---
# ------------------------------------------------------------------------
# Pod 1: kafka-cli (just a shell with Kafka+GCP dependencies)
# ------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: kafka-cli
  labels:
    app: kafka-cli
spec:
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""  # Use control-plane label for master
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: kafka-cli-container
      image: python:3.9-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          apt-get update && apt-get install -y gcc libatlas-base-dev &&
          pip install --no-cache-dir kafka-python google-cloud-storage matplotlib pillow &&
          tail -f /dev/null
  restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-cli-svc
spec:
  selector:
    app: kafka-cli
  ports:
    - name: kafka-cli-port
      protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30080
  type: NodePort

---
# ------------------------------------------------------------------------
# Pod 2: kafka-producer (runs the producer code automatically)
# ------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: kafka-producer
  labels:
    app: kafka-producer
spec:
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: kafka-producer-container
      image: python:3.9-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          apt-get update && apt-get install -y gcc libatlas-base-dev &&
          pip install --no-cache-dir kafka-python google-cloud-storage matplotlib pillow &&
          mkdir -p /app && cd /app &&
          cp /scripts/simulation_parameters.py . &&
          cp /scripts/kafka_producer.py . &&
          echo "Starting producer loop..." &&
          while true; do
            echo "Running producer..."
            python kafka_producer.py
            echo "Sleeping for 5 seconds..."
            sleep 15
          done
      volumeMounts:
        - name: scripts-volume
          mountPath: /scripts
          readOnly: true
  volumes:
    - name: scripts-volume
      hostPath:
        path: /home/asmae/kafka-scripts
  restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-producer-svc
spec:
  selector:
    app: kafka-producer
  ports:
    - name: kafka-producer-port
      protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30081
  type: NodePort

---
# ------------------------------------------------------------------------
# Pod 3: kafka-listener-1 (runs the consumer)
# ------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: kafka-listener-1
  labels:
    app: kafka-listener-1
spec:
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: kafka-listener-1-container
      image: python:3.9-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          apt-get update && apt-get install -y gcc libatlas-base-dev &&
          pip install --no-cache-dir kafka-python google-cloud-storage matplotlib pillow argparse &&
          mkdir -p /app && cd /app &&
          cp /scripts/simulation_parameters.py . &&
          cp /scripts/kafka_consumer.py . &&
          echo "Running consumer #1..." &&
          python kafka_consumer.py &&
          echo "Consumer #1 done. Keeping container alive." &&
          tail -f /dev/null
      volumeMounts:
        - name: scripts-volume
          mountPath: /scripts
          readOnly: true
        - name: gcp-cred-volume
          mountPath: /secrets
          readOnly: true
  volumes:
    - name: scripts-volume
      hostPath:
        path: /home/asmae/kafka-scripts
    - name: gcp-cred-volume
      secret:
        secretName: gcp-cred-secret
        items:
          - key: gcp_cred.json
            path: gcp_cred.json
  restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-listener-1-svc
spec:
  selector:
    app: kafka-listener-1
  ports:
    - name: listener1-port
      protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30082
  type: NodePort

---
# ------------------------------------------------------------------------
# Pod 4: kafka-listener-2 (runs the consumer again, or a second instance)
# ------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: kafka-listener-2
  labels:
    app: kafka-listener-2
spec:
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: kafka-listener-2-container
      image: python:3.9-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          apt-get update && apt-get install -y gcc libatlas-base-dev &&
          pip install --no-cache-dir kafka-python google-cloud-storage matplotlib pillow argparse &&
          mkdir -p /app && cd /app &&
          cp /scripts/simulation_parameters.py . &&
          cp /scripts/kafka_consumer.py . &&
          echo "Running consumer #2..." &&
          python kafka_consumer.py &&
          echo "Consumer #2 done. Keeping container alive." &&
          tail -f /dev/null
      volumeMounts:
        - name: scripts-volume
          mountPath: /scripts
          readOnly: true
        - name: gcp-cred-volume
          mountPath: /secrets
          readOnly: true
  volumes:
    - name: scripts-volume
      hostPath:
        path: /home/asmae/kafka-scripts
    - name: gcp-cred-volume
      secret:
        secretName: gcp-cred-secret
        items:
          - key: gcp_cred.json
            path: gcp_cred.json
  restartPolicy: Always
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-listener-2-svc
spec:
  selector:
    app: kafka-listener-2
  ports:
    - name: listener2-port
      protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30083
  type: NodePort

  ---
# ------------------------------------------------------------------------
# Pod 5: kafka-listener-3 (third consumer instance)
# ------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: kafka-listener-3
  labels:
    app: kafka-listener-3
spec:
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: kafka-listener-3-container
      image: python:3.9-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          apt-get update && apt-get install -y gcc libatlas-base-dev &&
          pip install --no-cache-dir kafka-python google-cloud-storage matplotlib pillow argparse &&
          mkdir -p /app && cd /app &&
          cp /scripts/simulation_parameters.py . &&
          cp /scripts/kafka_consumer.py . &&
          echo "Running consumer #3..." &&
          python kafka_consumer.py &&
          echo "Consumer #3 done. Keeping container alive." &&
          tail -f /dev/null
      volumeMounts:
        - name: scripts-volume
          mountPath: /scripts
          readOnly: true
        - name: gcp-cred-volume
          mountPath: /secrets
          readOnly: true
  volumes:
    - name: scripts-volume
      hostPath:
        path: /home/asmae/kafka-scripts
    - name: gcp-cred-volume
      secret:
        secretName: gcp-cred-secret
        items:
          - key: gcp_cred.json
            path: gcp_cred.json
  restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-listener-3-svc
spec:
  selector:
    app: kafka-listener-3
  ports:
    - name: listener3-port
      protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30084
  type: NodePort

---
# ------------------------------------------------------------------------
# Pod 6: kafka-listener-4 (fourth consumer instance)
# ------------------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: kafka-listener-4
  labels:
    app: kafka-listener-4
spec:
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  containers:
    - name: kafka-listener-4-container
      image: python:3.9-slim
      command: ["/bin/bash", "-c"]
      args:
        - |
          apt-get update && apt-get install -y gcc libatlas-base-dev &&
          pip install --no-cache-dir kafka-python google-cloud-storage matplotlib pillow argparse &&
          mkdir -p /app && cd /app &&
          cp /scripts/simulation_parameters.py . &&
          cp /scripts/kafka_consumer.py . &&
          echo "Running consumer #4..." &&
          python kafka_consumer.py &&
          echo "Consumer #4 done. Keeping container alive." &&
          tail -f /dev/null
      volumeMounts:
        - name: scripts-volume
          mountPath: /scripts
          readOnly: true
        - name: gcp-cred-volume
          mountPath: /secrets
          readOnly: true
  volumes:
    - name: scripts-volume
      hostPath:
        path: /home/asmae/kafka-scripts
    - name: gcp-cred-volume
      secret:
        secretName: gcp-cred-secret
        items:
          - key: gcp_cred.json
            path: gcp_cred.json
  restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-listener-4-svc
spec:
  selector:
    app: kafka-listener-4
  ports:
    - name: listener4-port
      protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30085
  type: NodePort